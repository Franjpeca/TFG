\chapter{Bibliografía}
\label{ch:bibliografia}



\begin{thebibliography}{9}

\bibitem{Raiaan2024}
\textit{M. A. K. Raiaan et al., "A Review on Large Language Models: Architectures, Applications, Taxonomies, Open Issues and Challenges," in IEEE Access, vol. 12, pp. 26839-26874, 2024, doi: 10.1109/ACCESS.2024.3365742. Disponible en: https://ieeexplore.ieee.org/document/10433480?denied=}

\bibitem{Vaswani2017}
\textit{Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. A., Kaiser, Ł., \& Polosukhin, I. (2017). Attention is all you need. Advances in Neural Information Processing Systems, 30. Disponible en https://arxiv.org/abs/1706.03762}

\bibitem{Devlin2018}
\textit{Devlin, J., Chang, M.-W., Lee, K., \& Toutanova, K. (2018). BERT: Pre-training of deep bidirectional transformers for language understanding. Proceedings of NAACL-HLT. Disponible en: https://arxiv.org/abs/1810.04805}

\bibitem{Gao2023}
\textit{Gao, Z.-F., Zhou, K., Liu, P., Zhao, W. X., \& Wen, J.-R. (2023). Small pre-trained language models can be fine-tuned as large models via over-parameterization. Disponible en: https://virtual2023.aclweb.org/paper_P295.html}

\bibitem{Golda}
\textit{A. Golda et al., "Privacy and Security Concerns in Generative AI: A Comprehensive Survey," in IEEE Access, vol. 12, pp. 48126-48144, 2024, doi: 10.1109/ACCESS.2024.3381611. Disponible en: https://ieeexplore.ieee.org/document/10478883}

\bibitem{Schick2021} 
\textit{Schick, T., \& Schütze, H. (2020). It's not just size that matters: Small language models are also few-shot learners. arXiv preprint arXiv:2009.07118. Disponible en: https://arxiv.org/abs/2009.07118}

\bibitem{Lu2024}
\textit{Lu, Z., Li, X., Cai, D., Yi, R., Liu, F., Zhang, X., Lane, N. D.,  Xu, M. (2024). Small Language Models: Survey, Measurements, and Insights. arXiv:2409.15790. Disponible en: https://arxiv.org/abs/2409.15790}

\bibitem{Lepagnol2024}
\textit{Lepagnol, E., Scialom, T., Abdou, W., Mathews, A., Behnke, S.,  Bosselut, A. (2024). Benchmarking Small Language Models for Zero-Shot Text Classification. arXiv:2404.11122. Disponible en: https://arxiv.org/abs/2404.11122.}

\bibitem{Hendrycks}
\textit{Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., \& Steinhardt, J. (2021). Measuring massive multitask language understanding. International Conference on Learning Representations (ICLR). Disponible en: https://arxiv.org/abs/2009.03300}

\bibitem{Zellers}
\textit{Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., \& Choi, Y. (2019). HellaSwag: Can a machine really finish your sentence? Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL). Disponible en: https://arxiv.org/abs/1905.07830}

\bibitem{Cobbe}
\textit{Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., … \& Schulman, J. (2021). Training verifiers to solve math word problems. Advances in Neural Information Processing Systems (NeurIPS). Disponible en: https://arxiv.org/abs/2110.14168}

\bibitem{Wang}
\textit{Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., \& Bowman, S. R. (2019). SuperGLUE: A stickier benchmark for general-purpose language understanding systems. Advances in Neural Information Processing Systems (NeurIPS). Disponible en: https://arxiv.org/abs/1905.00537}

\end{thebibliography}